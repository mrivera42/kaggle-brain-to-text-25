{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426381d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cb2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11/data_train.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8bf5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def load_h5py_file(file_path):\n",
    "    data = {\n",
    "        'neural_features': [],\n",
    "        'n_time_steps': [],\n",
    "        'seq_class_ids': [],\n",
    "        'seq_len': [],\n",
    "        'transcriptions': [],\n",
    "        'sentence_label': [],\n",
    "        'session': [],\n",
    "        'block_num': [],\n",
    "        'trial_num': [],\n",
    "    }\n",
    "    # Open the hdf5 file for that day\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "\n",
    "        keys = list(f.keys())\n",
    "\n",
    "        # For each trial in the selected trials in that day\n",
    "        for key in keys:\n",
    "            g = f[key]\n",
    "\n",
    "            neural_features = g['input_features'][:]\n",
    "            n_time_steps = g.attrs['n_time_steps']\n",
    "            seq_class_ids = g['seq_class_ids'][:] if 'seq_class_ids' in g else None\n",
    "            seq_len = g.attrs['seq_len'] if 'seq_len' in g.attrs else None\n",
    "            transcription = g['transcription'][:] if 'transcription' in g else None\n",
    "            sentence_label = g.attrs['sentence_label'][:] if 'sentence_label' in g.attrs else None\n",
    "            session = g.attrs['session']\n",
    "            block_num = g.attrs['block_num']\n",
    "            trial_num = g.attrs['trial_num']\n",
    "\n",
    "            data['neural_features'].append(neural_features)\n",
    "            data['n_time_steps'].append(n_time_steps)\n",
    "            data['seq_class_ids'].append(seq_class_ids)\n",
    "            data['seq_len'].append(seq_len)\n",
    "            data['transcriptions'].append(transcription)\n",
    "            data['sentence_label'].append(sentence_label)\n",
    "            data['session'].append(session)\n",
    "            data['block_num'].append(block_num)\n",
    "            data['trial_num'].append(trial_num)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7ef628",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_h5py_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b50360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neural_features',\n",
       " 'n_time_steps',\n",
       " 'seq_class_ids',\n",
       " 'seq_len',\n",
       " 'transcriptions',\n",
       " 'sentence_label',\n",
       " 'session',\n",
       " 'block_num',\n",
       " 'trial_num']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7502829",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(filepath, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f350a651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trial_0000',\n",
       " 'trial_0001',\n",
       " 'trial_0002',\n",
       " 'trial_0003',\n",
       " 'trial_0004',\n",
       " 'trial_0005',\n",
       " 'trial_0006',\n",
       " 'trial_0007',\n",
       " 'trial_0008',\n",
       " 'trial_0009',\n",
       " 'trial_0010',\n",
       " 'trial_0011',\n",
       " 'trial_0012',\n",
       " 'trial_0013',\n",
       " 'trial_0014',\n",
       " 'trial_0015',\n",
       " 'trial_0016',\n",
       " 'trial_0017',\n",
       " 'trial_0018',\n",
       " 'trial_0019',\n",
       " 'trial_0020',\n",
       " 'trial_0021',\n",
       " 'trial_0022',\n",
       " 'trial_0023',\n",
       " 'trial_0024',\n",
       " 'trial_0025',\n",
       " 'trial_0026',\n",
       " 'trial_0027',\n",
       " 'trial_0028',\n",
       " 'trial_0029',\n",
       " 'trial_0030',\n",
       " 'trial_0031',\n",
       " 'trial_0032',\n",
       " 'trial_0033',\n",
       " 'trial_0034',\n",
       " 'trial_0035',\n",
       " 'trial_0036',\n",
       " 'trial_0037',\n",
       " 'trial_0038',\n",
       " 'trial_0039',\n",
       " 'trial_0040',\n",
       " 'trial_0041',\n",
       " 'trial_0042',\n",
       " 'trial_0043',\n",
       " 'trial_0044',\n",
       " 'trial_0045',\n",
       " 'trial_0046',\n",
       " 'trial_0047',\n",
       " 'trial_0048',\n",
       " 'trial_0049',\n",
       " 'trial_0050',\n",
       " 'trial_0051',\n",
       " 'trial_0052',\n",
       " 'trial_0053',\n",
       " 'trial_0054',\n",
       " 'trial_0055',\n",
       " 'trial_0056',\n",
       " 'trial_0057',\n",
       " 'trial_0058',\n",
       " 'trial_0059',\n",
       " 'trial_0060',\n",
       " 'trial_0061',\n",
       " 'trial_0062',\n",
       " 'trial_0063',\n",
       " 'trial_0064',\n",
       " 'trial_0065',\n",
       " 'trial_0066',\n",
       " 'trial_0067',\n",
       " 'trial_0068',\n",
       " 'trial_0069',\n",
       " 'trial_0070',\n",
       " 'trial_0071',\n",
       " 'trial_0072',\n",
       " 'trial_0073',\n",
       " 'trial_0074',\n",
       " 'trial_0075',\n",
       " 'trial_0076',\n",
       " 'trial_0077',\n",
       " 'trial_0078',\n",
       " 'trial_0079',\n",
       " 'trial_0080',\n",
       " 'trial_0081',\n",
       " 'trial_0082',\n",
       " 'trial_0083',\n",
       " 'trial_0084',\n",
       " 'trial_0085',\n",
       " 'trial_0086',\n",
       " 'trial_0087',\n",
       " 'trial_0088',\n",
       " 'trial_0089',\n",
       " 'trial_0090',\n",
       " 'trial_0091',\n",
       " 'trial_0092',\n",
       " 'trial_0093',\n",
       " 'trial_0094',\n",
       " 'trial_0095',\n",
       " 'trial_0096',\n",
       " 'trial_0097',\n",
       " 'trial_0098',\n",
       " 'trial_0099',\n",
       " 'trial_0100',\n",
       " 'trial_0101',\n",
       " 'trial_0102',\n",
       " 'trial_0103',\n",
       " 'trial_0104',\n",
       " 'trial_0105',\n",
       " 'trial_0106',\n",
       " 'trial_0107',\n",
       " 'trial_0108',\n",
       " 'trial_0109',\n",
       " 'trial_0110',\n",
       " 'trial_0111',\n",
       " 'trial_0112',\n",
       " 'trial_0113',\n",
       " 'trial_0114',\n",
       " 'trial_0115',\n",
       " 'trial_0116',\n",
       " 'trial_0117',\n",
       " 'trial_0118',\n",
       " 'trial_0119',\n",
       " 'trial_0120',\n",
       " 'trial_0121',\n",
       " 'trial_0122',\n",
       " 'trial_0123',\n",
       " 'trial_0124',\n",
       " 'trial_0125',\n",
       " 'trial_0126',\n",
       " 'trial_0127',\n",
       " 'trial_0128',\n",
       " 'trial_0129',\n",
       " 'trial_0130',\n",
       " 'trial_0131',\n",
       " 'trial_0132',\n",
       " 'trial_0133',\n",
       " 'trial_0134',\n",
       " 'trial_0135',\n",
       " 'trial_0136',\n",
       " 'trial_0137',\n",
       " 'trial_0138',\n",
       " 'trial_0139',\n",
       " 'trial_0140',\n",
       " 'trial_0141',\n",
       " 'trial_0142',\n",
       " 'trial_0143',\n",
       " 'trial_0144',\n",
       " 'trial_0145',\n",
       " 'trial_0146',\n",
       " 'trial_0147',\n",
       " 'trial_0148',\n",
       " 'trial_0149',\n",
       " 'trial_0150',\n",
       " 'trial_0151',\n",
       " 'trial_0152',\n",
       " 'trial_0153',\n",
       " 'trial_0154',\n",
       " 'trial_0155',\n",
       " 'trial_0156',\n",
       " 'trial_0157',\n",
       " 'trial_0158',\n",
       " 'trial_0159',\n",
       " 'trial_0160',\n",
       " 'trial_0161',\n",
       " 'trial_0162',\n",
       " 'trial_0163',\n",
       " 'trial_0164',\n",
       " 'trial_0165',\n",
       " 'trial_0166',\n",
       " 'trial_0167',\n",
       " 'trial_0168',\n",
       " 'trial_0169',\n",
       " 'trial_0170',\n",
       " 'trial_0171',\n",
       " 'trial_0172',\n",
       " 'trial_0173',\n",
       " 'trial_0174',\n",
       " 'trial_0175',\n",
       " 'trial_0176',\n",
       " 'trial_0177',\n",
       " 'trial_0178',\n",
       " 'trial_0179',\n",
       " 'trial_0180',\n",
       " 'trial_0181',\n",
       " 'trial_0182',\n",
       " 'trial_0183',\n",
       " 'trial_0184',\n",
       " 'trial_0185',\n",
       " 'trial_0186',\n",
       " 'trial_0187',\n",
       " 'trial_0188',\n",
       " 'trial_0189',\n",
       " 'trial_0190',\n",
       " 'trial_0191',\n",
       " 'trial_0192',\n",
       " 'trial_0193',\n",
       " 'trial_0194',\n",
       " 'trial_0195',\n",
       " 'trial_0196',\n",
       " 'trial_0197',\n",
       " 'trial_0198',\n",
       " 'trial_0199',\n",
       " 'trial_0200',\n",
       " 'trial_0201',\n",
       " 'trial_0202',\n",
       " 'trial_0203',\n",
       " 'trial_0204',\n",
       " 'trial_0205',\n",
       " 'trial_0206',\n",
       " 'trial_0207',\n",
       " 'trial_0208',\n",
       " 'trial_0209',\n",
       " 'trial_0210',\n",
       " 'trial_0211',\n",
       " 'trial_0212',\n",
       " 'trial_0213',\n",
       " 'trial_0214',\n",
       " 'trial_0215',\n",
       " 'trial_0216',\n",
       " 'trial_0217',\n",
       " 'trial_0218',\n",
       " 'trial_0219',\n",
       " 'trial_0220',\n",
       " 'trial_0221',\n",
       " 'trial_0222',\n",
       " 'trial_0223',\n",
       " 'trial_0224',\n",
       " 'trial_0225',\n",
       " 'trial_0226',\n",
       " 'trial_0227',\n",
       " 'trial_0228',\n",
       " 'trial_0229',\n",
       " 'trial_0230',\n",
       " 'trial_0231',\n",
       " 'trial_0232',\n",
       " 'trial_0233',\n",
       " 'trial_0234',\n",
       " 'trial_0235',\n",
       " 'trial_0236',\n",
       " 'trial_0237',\n",
       " 'trial_0238',\n",
       " 'trial_0239',\n",
       " 'trial_0240',\n",
       " 'trial_0241',\n",
       " 'trial_0242',\n",
       " 'trial_0243',\n",
       " 'trial_0244',\n",
       " 'trial_0245',\n",
       " 'trial_0246',\n",
       " 'trial_0247',\n",
       " 'trial_0248',\n",
       " 'trial_0249',\n",
       " 'trial_0250',\n",
       " 'trial_0251',\n",
       " 'trial_0252',\n",
       " 'trial_0253',\n",
       " 'trial_0254',\n",
       " 'trial_0255',\n",
       " 'trial_0256',\n",
       " 'trial_0257',\n",
       " 'trial_0258',\n",
       " 'trial_0259',\n",
       " 'trial_0260',\n",
       " 'trial_0261',\n",
       " 'trial_0262',\n",
       " 'trial_0263',\n",
       " 'trial_0264',\n",
       " 'trial_0265',\n",
       " 'trial_0266',\n",
       " 'trial_0267',\n",
       " 'trial_0268',\n",
       " 'trial_0269',\n",
       " 'trial_0270',\n",
       " 'trial_0271',\n",
       " 'trial_0272',\n",
       " 'trial_0273',\n",
       " 'trial_0274',\n",
       " 'trial_0275',\n",
       " 'trial_0276',\n",
       " 'trial_0277',\n",
       " 'trial_0278',\n",
       " 'trial_0279',\n",
       " 'trial_0280',\n",
       " 'trial_0281',\n",
       " 'trial_0282',\n",
       " 'trial_0283',\n",
       " 'trial_0284',\n",
       " 'trial_0285',\n",
       " 'trial_0286',\n",
       " 'trial_0287']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43759287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.21 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.28 ['data_train.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.28 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.30 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.13 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.24 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.10 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.08 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.27 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.03 ['data_train.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.29 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.06 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.01 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.17 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.25 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.17 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.13 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.04 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.15 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.03 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.03 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.15 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.19 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.03 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.17 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.14 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.08 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.26 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.20 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.05.10 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.06.14 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.19 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.18 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2025.04.13 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.16 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.08 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.12 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.25 ['data_train.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.29 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.22 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11 ['data_train.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2024.02.25 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.10 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.20 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n",
      "t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.01 ['data_test.hdf5', 'data_train.hdf5', 'data_val.hdf5']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "for folder, _, files in os.walk('t15_copyTask_neuralData/hdf5_data_final'):\n",
    "    \n",
    "    if 'data_train.hdf5' in files:\n",
    "\n",
    "        print(folder, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2312d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class NeuralDataset(torch.utils.data.Dataset): \n",
    "\n",
    "    def __init__(self, dir):\n",
    "        \n",
    "        self.data = {\n",
    "            'neural_features': [],\n",
    "            'n_time_steps': [],\n",
    "            'seq_class_ids': [], \n",
    "            'seq_len': [], \n",
    "            'transcription': [], \n",
    "            'sentence_label': [], \n",
    "            'session': [], \n",
    "            'block_num': [], \n",
    "            'trial_num': []\n",
    "        }\n",
    "\n",
    "        for folder, __, files in os.walk(dir): \n",
    "\n",
    "            if 'data_train.hdf5' in files: \n",
    "\n",
    "                # load file \n",
    "                f = h5py.File(os.path.join(folder, 'data_train.hdf5'))\n",
    "\n",
    "                # loop through trials \n",
    "                for i in list(f.keys()): \n",
    "\n",
    "                    trial = f[i]\n",
    "\n",
    "                    neural_features = trial['input_features'][:]\n",
    "                    n_time_steps = trial.attrs['n_time_steps']\n",
    "                    seq_class_ids = trial['seq_class_ids'][:] if 'seq_class_ids' in trial else None\n",
    "                    seq_len = trial.attrs['seq_len'] if 'seq_len' in trial.attrs else None\n",
    "                    transcription = trial['transcription'][:] if 'transcription' in trial else None\n",
    "                    sentence_label = trial.attrs['sentence_label'][:] if 'sentence_label' in trial.attrs else None\n",
    "                    session = trial.attrs['session']\n",
    "                    block_num = trial.attrs['block_num']\n",
    "                    trial_num = trial.attrs['trial_num']\n",
    "\n",
    "                    # append trial features to data list \n",
    "                    self.data['neural_features'].append(neural_features)\n",
    "                    self.data['n_time_steps'].append(n_time_steps)\n",
    "                    self.data['seq_class_ids'].append(seq_class_ids)\n",
    "                    self.data['seq_len'].append(seq_len)\n",
    "                    self.data['transcription'].append(transcription)\n",
    "                    self.data['sentence_label'].append(sentence_label)\n",
    "                    self.data['session'].append(session)\n",
    "                    self.data['block_num'].append(block_num)\n",
    "                    self.data['trial_num'].append(trial_num)\n",
    "\n",
    "    def __len__(self): \n",
    "\n",
    "        return len(self.data['neural_features'])\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "\n",
    "        return {\n",
    "            'neural_features': torch.tensor(self.data['neural_features'][idx]),\n",
    "            'n_time_steps': torch.tensor(self.data['n_time_steps'][idx]),\n",
    "            'seq_class_ids': torch.tensor(self.data['seq_class_ids'][idx]),\n",
    "            'seq_len': torch.tensor(self.data['seq_len'][idx]),\n",
    "            'transcription': self.data['transcription'][idx],\n",
    "            'sentence_label': self.data['sentence_label'][idx],\n",
    "            'session': self.data['session'][idx],\n",
    "            'block_num': self.data['block_num'][idx],\n",
    "            'trial_num': self.data['trial_num'][idx]\n",
    "        }\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a999eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "brainDataset = NeuralDataset('t15_copyTask_neuralData/hdf5_data_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22995e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([517, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brainDataset.__getitem__(4)['neural_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5031be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55ad6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader \n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "import numpy as np\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # gather all the variable length items (neural_features, seq_labels) from the batch into separate python lists first so we can find the max length of each group and pad each group together\n",
    "\n",
    "    neural_features = [i['neural_features'] for i in batch]\n",
    "    seq_class_ids = [i['seq_class_ids'] for i in batch]\n",
    "    n_time_steps = [i['n_time_steps'] for i in batch]\n",
    "    seq_len = [i['seq_len'] for i in batch] \n",
    "    transcription = [i['transcription'] for i in batch]\n",
    "    sentence_label = [i['sentence_label'] for i in batch]\n",
    "    session = [i['session'] for i in batch]\n",
    "    block_num = [i['block_num'] for i in batch]\n",
    "    trial_num = [i['trial_num'] for i in batch]\n",
    "    # neural_lengths = [len(i) for i in neural_features]\n",
    "    # seq_class_lengths = [len(i) for i in seq_class_ids]\n",
    "\n",
    "    # max_neural_idx = np.argmax(neural_lengths)\n",
    "    # max_seq_class_idx = np.argmax(seq_class_lengths)\n",
    "\n",
    "    # max_neural_len = neural_lengths[max_neural_idx]\n",
    "    # max_seq_len = seq_class_lengths[max_seq_class_idx]\n",
    "\n",
    "    neural_features_padded = pad_sequence(neural_features, batch_first=True, padding_value=0)\n",
    "    seq_class_ids_padded = pad_sequence(seq_class_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'neural_features': neural_features_padded,\n",
    "        'seq_class_ids': seq_class_ids_padded,\n",
    "        'n_time_steps': n_time_steps,\n",
    "        'seq_len': seq_len,\n",
    "        'transcription': transcription,\n",
    "        'sentence_label': sentence_label,\n",
    "        'session': session,\n",
    "        'block_num': block_num,\n",
    "        'trial_num': trial_num\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e383f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainLoader = DataLoader(brainDataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a130bbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neural_features': tensor([[[-0.3003,  0.4238, -0.7064,  ..., -0.3712, -0.3446,  0.2089],\n",
       "          [-0.3003, -0.7120, -0.7064,  ...,  3.3335,  0.9658, -0.3117],\n",
       "          [-0.3003, -0.7120, -0.7064,  ...,  0.9588, -1.1609,  1.6493],\n",
       "          ...,\n",
       "          [-0.3003, -0.7120,  3.0599,  ..., -0.8327,  1.0939, -0.1702],\n",
       "          [-0.3003, -0.7120,  0.5490,  ..., -0.9575, -0.3140, -1.1289],\n",
       "          [-0.3003, -0.7120, -0.7064,  ...,  0.0992, -1.4184,  0.5835]],\n",
       " \n",
       "         [[-0.2886, -0.6776,  0.6852,  ..., -0.8004,  0.0621,  0.5236],\n",
       "          [-0.2886,  0.5355,  3.3447,  ...,  1.2232,  0.5247, -0.9197],\n",
       "          [-0.2886,  0.5355, -0.6445,  ..., -0.2722, -0.6868, -0.5445],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.2746,  0.5775, -0.6807,  ...,  0.9687, -0.0815,  0.6085],\n",
       "          [-0.2746, -0.6481,  1.8365,  ..., -0.5639,  0.7008,  1.1431],\n",
       "          [-0.2746, -0.6481, -0.6807,  ..., -0.7827,  0.0639, -1.2594],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.2716, -0.6396, -0.6780,  ..., -0.8097, -0.5040, -1.2761],\n",
       "          [-0.2716, -0.6396, -0.6780,  ...,  0.3748,  0.7030,  0.1488],\n",
       "          [-0.2716, -0.6396, -0.6780,  ..., -0.7618, -0.9032, -0.2469],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]),\n",
       " 'seq_class_ids': tensor([[ 3, 40,  7,  ...,  0,  0,  0],\n",
       "         [16,  5, 40,  ...,  0,  0,  0],\n",
       "         [22,  6, 23,  ...,  0,  0,  0],\n",
       "         [ 6, 22, 40,  ...,  0,  0,  0]], dtype=torch.int32),\n",
       " 'n_time_steps': [tensor(654), tensor(524), tensor(488), tensor(458)],\n",
       " 'seq_len': [tensor(34), tensor(21), tensor(19), tensor(22)],\n",
       " 'transcription': [array([ 65,  32,  98, 117, 110,  99, 104,  32, 111, 102,  32, 112, 115,\n",
       "         121,  99, 104, 105,  97, 116, 114, 105, 115, 116, 115,  32,  97,\n",
       "         114, 101,  32, 104,  97, 110, 103, 105, 110, 103,  32, 111, 117,\n",
       "         116,  46,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0], dtype=int32),\n",
       "  array([ 72, 111, 119,  32, 100, 105, 100,  32, 121, 111, 117,  32, 106,\n",
       "         111, 105, 110,  32, 116, 104, 101,  32, 102, 105, 103, 104, 116,\n",
       "          63,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0], dtype=int32),\n",
       "  array([ 77, 105, 110, 101,  32,  97, 114, 101,  32, 109, 105, 120, 101,\n",
       "         100,  32, 117, 112,  32, 116, 111, 111,  46,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0], dtype=int32),\n",
       "  array([ 73,  39, 109,  32,  97, 110,  32, 101, 110, 103, 105, 110, 101,\n",
       "         101, 114,  32,  98, 121,  32, 116, 114,  97, 100, 101,  46,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0], dtype=int32)],\n",
       " 'sentence_label': ['A bunch of psychiatrists are hanging out.',\n",
       "  'How did you join the fight?',\n",
       "  'Mine are mixed up too.',\n",
       "  \"I'm an engineer by trade.\"],\n",
       " 'session': ['t15.2024.07.21',\n",
       "  't15.2024.07.21',\n",
       "  't15.2024.07.21',\n",
       "  't15.2024.07.21'],\n",
       " 'block_num': [1, 1, 1, 1],\n",
       " 'trial_num': [0, 1, 2, 3]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03687bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOGIT_TO_PHONEME = [\n",
    "   # \"BLANK\" = CTC blank symbol\n",
    "'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "'AY', 'B', 'CH', 'D', 'DH',\n",
    "'EH', 'ER', 'EY', 'F', 'G',\n",
    "'HH', 'IH', 'IY', 'JH', 'K',\n",
    "'L', 'M', 'N', 'NG', 'OW',\n",
    "'OY', 'P', 'R', 'S', 'SH',\n",
    "'T', 'TH', 'UH', 'UW', 'V',\n",
    "'W', 'Y', 'Z', 'ZH',\n",
    "' | ',    # \"|\" = silence token\n",
    "] \n",
    "\n",
    "len(LOGIT_TO_PHONEME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b1d33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b22c4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaselineLSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # input (B x T x 512) --> output (B x T x 41)\n",
    "\n",
    "        self.rnn = torch.nn.LSTM(input_size=512,hidden_size=768, num_layers=5)\n",
    "        self.proj = torch.nn.Linear(in_features=768, out_features=41)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        # print('rnn input: ', x.shape)\n",
    "        x, _ = self.rnn(x)\n",
    "        # print('linear input: ', x.shape)\n",
    "        x = F.log_softmax(self.proj(x),dim=2)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "602ffb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineLSTM().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d1b2915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineLSTM(\n",
      "  (rnn): LSTM(512, 768, num_layers=5)\n",
      "  (proj): Linear(in_features=768, out_features=41, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899ad3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m optimizer.zero_grad()\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# forward \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# print(f'output shape: {output.shape}' )\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# compute loss \u001b[39;00m\n\u001b[32m     35\u001b[39m loss = loss_fn(output, targets, input_lengths, target_lengths)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mBaselineLSTM.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# print('rnn input: ', x.shape)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     x, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# print('linear input: ', x.shape)\u001b[39;00m\n\u001b[32m     21\u001b[39m     x = F.log_softmax(\u001b[38;5;28mself\u001b[39m.proj(x),dim=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/kaggle-brain-to-text-25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/kaggle-brain-to-text-25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/kaggle-brain-to-text-25/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1127\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1124\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1139\u001b[39m     result = _VF.lstm(\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1141\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1148\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1149\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# define training loop \n",
    "import torch.optim as optim \n",
    "\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CTCLoss()\n",
    "num_epochs = 2\n",
    "\n",
    "model.train()\n",
    "\n",
    "epoch_loss = 0\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    \n",
    "    train_loss = 0\n",
    "    num_batches = len(trainLoader)\n",
    "    for i, batch in enumerate(trainLoader):\n",
    "\n",
    "        # place tensors on device\n",
    "        inputs, targets = batch['neural_features'].to(device), batch['seq_class_ids'].to(device)\n",
    "        inputs = torch.transpose(inputs, 0, 1)\n",
    "        # print(f'inputs shape: {inputs.shape}')\n",
    "        # print(f'targets shape: {targets.shape}')\n",
    "        input_lengths, target_lengths = torch.tensor(batch['n_time_steps']).to(device), torch.tensor(batch['seq_len']).to(device)\n",
    "        # print(f'input_lengths shape: {input_lengths.shape}')\n",
    "        # print(f'target_lengths shape: {target_lengths.shape}')\n",
    "\n",
    "        # zero optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        output = model.forward(inputs)\n",
    "        # print(f'output shape: {output.shape}' )\n",
    "\n",
    "        # compute loss \n",
    "        loss = loss_fn(output, targets, input_lengths, target_lengths)\n",
    "        train_loss += loss.item()\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # backprop \n",
    "        loss.backward()\n",
    "\n",
    "        # update weights \n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= num_batches \n",
    "    print(f'Epoch {epoch} loss: {train_loss}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7310a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = brainDataset.__getitem__(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed97a951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You can't make a decision.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([chr(int(i)) for i in sample['transcription']]).replace('\\x00','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "342a0a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AA'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOGIT_TO_PHONEME[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c3193be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Y',\n",
       " 'UW',\n",
       " ' | ',\n",
       " 'K',\n",
       " 'AE',\n",
       " 'N',\n",
       " 'T',\n",
       " ' | ',\n",
       " 'M',\n",
       " 'EY',\n",
       " 'K',\n",
       " ' | ',\n",
       " 'AH',\n",
       " ' | ',\n",
       " 'D',\n",
       " 'IH',\n",
       " 'S',\n",
       " 'IH',\n",
       " 'ZH',\n",
       " 'AH',\n",
       " 'N',\n",
       " ' | ',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK',\n",
       " 'BLANK']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[LOGIT_TO_PHONEME[i] for i in sample['seq_class_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "806bd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bc847e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /home/max-\n",
      "[nltk_data]     rivera/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d17754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f296a9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K', 'AA1', 'R', 'N', 'IH0', 'V', 'AO2', 'R']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['carnivore'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "361fb335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_21845/3567647354.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  [i.replace('\\d+','') for i in d['shoe'][0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SH', 'UW1']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.replace('\\d+','') for i in d['shoe'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "120c5953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UW'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "s = 'UW1'\n",
    "\n",
    "re.sub(r'\\d','',s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d060ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SH', 'UW']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.sub(r'\\d','',s) for s in d['shoe'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f4dda62",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIT_TO_PHONEME = [\n",
    "\"BLANK\",# \"BLANK\" = CTC blank symbol\n",
    "'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "'AY', 'B', 'CH', 'D', 'DH',\n",
    "'EH', 'ER', 'EY', 'F', 'G',\n",
    "'HH', 'IH', 'IY', 'JH', 'K',\n",
    "'L', 'M', 'N', 'NG', 'OW',\n",
    "'OY', 'P', 'R', 'S', 'SH',\n",
    "'T', 'TH', 'UH', 'UW', 'V',\n",
    "'W', 'Y', 'Z', 'ZH',\n",
    "# ' | ',    # \"|\" = silence token\n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9ee3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode import build_ctcdecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "554f8b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/max-rivera/git/kaggle-brain-to-text-25/4-gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Space token ' ' missing from vocabulary.\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "decoder = build_ctcdecoder(\n",
    "    labels=LOGIT_TO_PHONEME,\n",
    "    kenlm_model_path=\"4-gram.arpa\",\n",
    "    alpha=0.4,\n",
    "    beta=1.6,\n",
    "    unigrams=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f8f4891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:  I grew up in Alabama.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "torch.Size([857, 512])\n",
      "torch.Size([1, 857, 512])\n",
      "RUHR\n"
     ]
    }
   ],
   "source": [
    "# infer on a sample \n",
    "sample = brainDataset.__getitem__(25)\n",
    "print('sample: ', ''.join([chr(i) for i in sample['transcription']]))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    inputs, targets = sample['neural_features'].to(device), sample['seq_class_ids'].to(device)\n",
    "\n",
    "    print(inputs.shape)\n",
    "\n",
    "    inputs = torch.unsqueeze(inputs,0)\n",
    "\n",
    "    print(inputs.shape)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    logits = outputs[0]  # or outputs.logits[0] depending on your model output\n",
    "\n",
    "    logits_np = logits.detach().cpu().numpy()\n",
    "\n",
    "    print(decoder.decode(logits_np,beam_width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e53a2d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape: torch.Size([857, 1, 41])\n",
      "logits.shape: torch.Size([857, 41])\n",
      "decode (log-probs): R\n",
      "decode (probs): R\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # sample['neural_features'] has shape [T, features]\n",
    "    inputs = sample['neural_features'].to(device)          # -> [T, 512]\n",
    "    inputs = inputs.unsqueeze(1)                          # -> [T, 1, 512] (seq_len, batch=1, feat)\n",
    "    outputs = model(inputs)                              # -> [T, 1, num_labels]\n",
    "    logits = outputs[:, 0, :]                            # -> [T, num_labels]\n",
    "    logits_np = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Try decode with log-probs first, if it looks off try np.exp(logits_np)\n",
    "    print(\"outputs.shape:\", outputs.shape)\n",
    "    print(\"logits.shape:\", logits.shape)\n",
    "    print(\"decode (log-probs):\", decoder.decode(logits_np, beam_width=50))\n",
    "    print(\"decode (probs):\", decoder.decode(np.exp(logits_np), beam_width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add5e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-brain-to-text-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
